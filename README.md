# On Neural Expressivity 

This repository aims to search for the neural network expressivity, how the 
architectural properties of a neural network - such as depth, width, layer
type - could affect the resulting functions computational property.

This repository mainly rooted on the recently published article, [On the 
Expressive Power of Deep Neural Networks (Raghu et al, 2017)](http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf)

## Table of Contents 
1. Universal Approximation Theorem 

2. On Lower Bounds and Upper Bound 
  
     To evade the waste of computation, to find this upper and lower bound is
     practically important, however still not enough literature has been 
     published.
     
     Moreover, formalization also matters. How to define the *Bounds of 
     Neural Expressivity* is still unanswered clearly.
     
3. Measure of Expressivity
    
    __Currently known measures__
     
    - *Activation Pattern* (Raghu et al. 2017)
    
        Using the notion of *transition*, where changing an input x to 
        nearby point x + \sigma 



## References

Raghu, M., Poole, B., Kleinberg, J., Ganguli, S. & Sohl-Dickstein, J.. (2017). On the Expressive Power of Deep Neural Networks. Proceedings of the 34th International Conference on Machine Learning, in PMLR 70:2847-2854
